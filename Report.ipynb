{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Game DDQN in Unity Environment\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This repo implements a Doulbe Deep Q Learning Network agent in Python3 and PyTorch that learns to play the Banana Navigation game in a Unity environment. The goal of the game is to pick up yellow bananas, while avoiding blue bananas. The agent receives +1 points for yellow bananas and -1 point for  blue bananas.  The game is considered \"solved\" by the agent when it acheives an average score of at least 13 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "This Deep Q-Network (DQN) algorithm in this repo is based on work first proposed in a paper titled, _Human-level control through deep reinforcement learning_.\n",
    "\n",
    "A few key aspects of DQN outlined in the paper are listed below: \n",
    "\n",
    "- DQN is a deep convolutional neural network that takes the raw video pixels and the game score as input, and maps to a fully-connected output with softmax activation and number of units equal to the action space of the game.\n",
    "\n",
    "- DQN is a technique to estimate the optimal action-value function given below.\n",
    "![action-value function](av_function.PNG)  \n",
    "\n",
    "- DQN uses a method called _experience replay_, where a buffer of experiences (S, A, R, S') are stored and drawn from uniformly at random in minibatches during each network update.\n",
    "  \n",
    "  \n",
    "- DQN uses a method called _fixed Q target_, where 2 identical networks are used; one called the local network is the agent being trained, while another, called the target network gets updated less frequently or more slowly and is used as the target value in the loss function, which stabalizes and prevents divergence of the network during training.\n",
    "\n",
    "\n",
    "- Below is the loss function used, in which theta represents the local network, and theta-superscript-minus represetents the target network.\n",
    "![loss-function](loss_func.PNG)\n",
    "\n",
    "- The DQN algorithm pseudocode is given below.  \n",
    "![algorithm](algo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Network Achitecture\n",
    "\n",
    "The architecture in this implementation is as follows:\n",
    "\n",
    "Input:     Linear(num_units = state size)  \n",
    "Hidden 1:  Linear(num_units = 256) > ReLU  \n",
    "Hidden 2:  Linear(num_units = 128) > ReLU  \n",
    "Hidden 3:  Linear(num_units = 64)  > ReLU  \n",
    "Output:    Linear(num_units = action size)\n",
    "\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "|Hyperparameter|Value|Description|\n",
    "|--------------:|----:|:-----------|\n",
    "|minibatch size| 64 | Number of training examples to sample from memory|\n",
    "|replay buffer|100000|Number of experiences to store in memory|\n",
    "|gamma|0.99|Discount factor of gamma used in Q-learning update|\n",
    "|update frequency|4|how ofter to update network weights|\n",
    "|target network update frequency|4|how ofter to update target network weights|\n",
    "|learning rate|0.0005|The learning rate used by Adam|\n",
    "|tau|0.001|The parameters used by soft update of target network weights|\n",
    "|training episodes|1000|Number of episodes used for training|\n",
    "|starting epsilon|1.0|Starting epsilon used by eps-greedy policy|\n",
    "|epsilon decay|0.975|Decay rate of epsilon|\n",
    "|minimum epsilon|0.005|Minimum value of epsilon|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards  \n",
    "\n",
    "The plot below illustrates the agent's performance over time (or number of episodes). In the case of Banana's the game is considered solved when the agent acheives an average score of at least 13.0 over 100 consecutive episodes. We can see that our **Double Deep Q-Network agent was able to solve the game after 246 episodes**\n",
    "\n",
    "![Performance](solved.PNG)\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "\n",
    "The implementation from this report already has one extension of the original Deep Q-Network algorithm, which is the _Double Deep Q-Network_ algorithm.\n",
    "\n",
    "Additional thoughts for future improvements that have been identified in literature and would be logical next steps to improve the agent, include:  \n",
    "\n",
    "1. **Priortized Experience Replay**: rather than a uniform random selection of experiences to learn from, we would assign weighted probabilities to the experiences in order to prioritize experences with the greater losses, for example...  \n",
    "\n",
    "2. **Other Variations of Experience Replay**: like increases the buffer, or imposing a minimum loss before adding to the buffer.  \n",
    "3. **A Dueling DQN**: where the DQN architecture branches at the end to and recombines via a formula to the state value and the state-action values represented by each side of the branched network.    \n",
    "\n",
    "4. **A Convolutional DQN**: where instead of taking a 37 dimensional state space as input, we accept the raw pixels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
